\section{Example}
\label{sec:example}

\mc{Figure ref{fig:} shows the result for}

Looking at Shazam's \Flaky Test monitor for the last 10 nightly builds is
interesting. By manual inspection, we can identify a good \flaky test candidate.

The nightly Jenkins plan runs all tests across 7 devices. Some of these are
tablets, and some phones. All have different resolutions, hardware
specifications and api levels.

Shazam's \Flaky Test Monitor gives us a historical view of the acceptance test
results of the past 10 builds. A subset of the most frequently failing
acceptance tests are shown, split by test and device. We are able to manually
infer flakiness by inspecting each row and column in isolation.

Almost immediately, we find a classic example.

com.shazam.android.acceptancetests.musicdetails.details.GenericDetailsAdaptiveInteractiveInfoTest.testThatShareIsAbsent:

Over the past 10 builds (\#572-581), this test has been run 70 times across 7
devices. During these test runs, it has failed once. This gives us a probability
of failure of:
$$ 1 / 70 * 100 = 1.43\% $$

\subsection{Manually Debugging}

We note that the failure occured on build number 577.

Looking up the column, we see that a few other tests failed during that build on
the same device (015d3b6664601604\footnote{We can map the GUID back to a
physical device during debugging if needs be.}).

The next step would be to look at the test result page for that device and test.
Thankfully, the \Flaky Test Monitor is integrated with the test runner results,
and links us straight there.

We get a full logcat log file, and a screenshot of the device taken at the point
of failure. This is typical of an acceptance test runner. \todo{Is other
information common?}.

Inspecting the logcat \todo{What is logcat?} for the test run, we find the
stacktrace:

\begin{alltt}
java.lang.AssertionError:
Expected:  Bitmap: android.graphics.Bitmap@426bbd18
but:
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
...\footnote{Rest of stacktrace ommitted to save space.}
\end{alltt}

At this point, we have a stacktrace and a screenshot:

\begin{itemize}
	\item The stacktrace is fairly useless at this point. What is
	\quote{Bitmap@426bbd18}?
	\item Presuming the test is named correctly
	({\lq}testThatShareIsAbsent{\rq}), the screenshot seems to be in order --- we
	can see no {\lq}Share{\rq} icon on the page.
\end{itemize}

From this point, multiple debugging routes could be taken. A developer could
chose to:

\begin{itemize}
	\item Inspect screenshots from passing runs on the same device. Perhaps an
	obvious display difference will point them in the right direction.
	\item Jump straight to the relevant code (\ie, the test source code) and read
	through to ensure no obvious points of potential failure are present.
	\item Inspect the logcat log manually, perhaps comparing it to successful runs
	on the same (or different) devices. Perhaps a giveaway log statement can be
	found that will lead to a relevant code section.
	\item \todo{\etc...}
\end{itemize}

These options are limited. What if we had been running \splatter?
