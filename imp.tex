% Focus on the interesting design decisions. For example, what were the alternatives, why select one particular solution?
% Don't flood the chapter with diagrams. Be selective.
% Avoid lengthy sections of code; use pseudo-code.
% This is a core chapter and will usually be quite substantial, 10 pages or more.
\section{Design and Implementation}
\label{sec:imp}

\begin{mdframed}
	\begin{itemize}
		\item Describe the design of what you have created.
		\item Start with the application architecture, giving its overall structure and the components that make up that structure.
		\item Give a description of the design of each of the components that make up the architecture.
		\item Include the database or storage representation.
		\item Provide implementation details as necessary.
	\end{itemize}
\end{mdframed}


This section details the design and implementation of our proof of concept tool, as it stands so far. It is comprised of two parts: a Jenkins plugin and an Android instrumentation.


\subsection{Jenkins Plugin}

The Jenkins plugin is a wrapper that manages the storage of the test results and associated artifacts in a HSQL database.


\subsection{Android Instrumentation}

The repository is made up of a few projects. Figure \ref{fig:repo_structure} shows an overview of the structure and key submodules.

\begin{figure}[h]
    \dirtree{%
    .1 heisentest-splatter.
    .2 dex-generator\DTcomment{A basic Android application from which ASMDEX visitor code is generated}.
    .2 dexifier\DTcomment{A Java command line tool that generates ASMDEX visitor code from an APK}.
    .2 skeleton-android-app\DTcomment{A basic Android application, with Espresso\todo{cite}-based tests}.
    .2 splatter-instrumentation\DTcomment{Java command line tool responsible for the actual instrumentation}.
    .2 splatter-sdk\DTcomment{Java library that defines the {\tt @Splatter} annotation}.
    }
\caption{}
\label{fig:repo_structure}
\end{figure}

The main tool --- \splatterinst{} --- is Java-based and operates from the command line.

\subsubsection{Command Line Arguments}

It takes a number of arguments, all necessary for the correct running of the program. If any of the necessary arguments are missing, a usage message is printed and the program exits.

The following arguments are required:
\begin{itemize}
    \item \texttt{applicationApk} --- The fully qualified path to the Application APK to be instrumented.
    \item \texttt{testApk} ---The fully qualified path to the Application Test APK.
    \item \texttt{androidPlatforms} --- The fully qualified path to an \texttt{Android SDK/platforms/} folder.
\end{itemize}

An example usage, run from the directory containing the APKs to be instrumented, would look something like:

\texttt{splatter --applicationApk=android-app-debug.apk\\--testApk=android-app-debug-test.apk\\--androidPlatforms=\$ANDROID\_HOME/android-platforms}

We use the Apache commons-cli library for command line argument parsing and validation.

\subsubsection{Soot}

We looked into Soot for its ability to generate control flow graphs (CFGs). When searching for failure-predicting predicates, we will favor locations futher down the CFG when placing probes (new or moved). Section \todo{ref} describes this process in more detail.

It is possible to generate the CFG for a method using ASMDEX alone, although it is a slightly more tricky task.

In the end, although Soot was a nice tool to work with, we scrapped it for a few reasons. Firstly, the representation used by Soot is fairly incompatible with that of ASMDEX. Even if we could determine the CFG for a method using Soot, we'd have to somehow map the locations to our ASMDEX instrumentation sites --- a non-trivial task. The overhead of starting up Soot and loading an APK is significant. On an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-3610QM CPU @ 2.30GHz, the the time to load the application and test APKs was 10.76 seconds. With a total time of 19.09 seconds, Soot takes up 56\% of the total instrumentation time (and that is \textit{without} generating the CFGs for any methods). In fact, with Soot removed entirely, the same instrumentation run took just 3.294 seconds --- 17\% of the previous total run time.

\subsubsection{Hardcoded Strings}

There are various points in the application that have been hardcoded during development and never generalised. These are marked with \texttt{// TODO:} comments in the places they do exist. Obviously, these dependencies must be pulled up into the \texttt{Options} command line arguments, so that a user can modify them.

\subsubsection{Future Work}

The plugin will need to analyse historical test results and calculate the probability of failure for each test \todo{as discuss in section $n$}. It will also need to be extended with an interface to visualise test flakiness and related information {\todo bring in some of the information from approach here, flakiness tables \etc}. This is also related to prioritisation.


\subsection{A Choice of Continuous Integration Tools}

Jenkins \cite{Jenkins} is a popular open source continuous integration tool. Like others of its kind, it maintains a history of build results and their associated artifacts. A wealth of plugins provide support for testing frameworks such as JUnit --- displaying individual test outcomes, stack traces and assertions in a simple interface.

Although there are many open source alternatives which we could support (\eg, Hudson, from which the Jenkins project derives), many of these have significantly smaller user bases and commit activity. Mostly though, our decision to support Jenkins was motivated by Shazam's use of it across all teams.

Although the tool will be developed initially for Jenkins, it is essential to structure the plugin such that it can easily be migrated to other CI systems. At all stages of design, this must be kept in mind.


\subsection{A Solution}

\subsubsection{Identification}

This stage is fairly trivial since we can build upon the work of others.

After a test run completes, we can gather the test run results and store them in a persistent database. We can then look at the test runs over time and attach various values to each test much in the same way as the existing tools. ‘Flakiness’ - a percentage value representing a test’s likelihood to fail will be the most useful of these.
TODO: Write a list of values we hope to attach to each test. Back this up with quotes from Shazam.

\subsubsection{Prioritisation}

We can use the data gathered during the identification stage to rank the tests according to criteria. A simple output would be a descending list ordered by flakiness:

\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    Test & Flakiness \% \\ \hline
    testVeryFlaky() & 91 \\ \hline
    testSometimesFlaky() & 33 \\ \hline
    testSignificantlyFlaky() & 2 \\ \hline
    \end{tabular}
\end{center}

Or, we could group the tests by common failure type, e.g.:

% \lstset{
% 	numbers=none,
% 	xleftmargin=3pt,
% 	numbersep=1em
% }

\begin{center}
    \begin{tabular}{| p{10cm} | l |}
    \hline
    Common Failure & \flaky{} Test Count \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
java.lang.OutOfMemoryError
	\end{lstlisting}}
    & 4 \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
java.lang.AssertionError:
Expected: (View visibility to be View.VISIBLE and View to have a width and a height)
but: View visibility to be View.VISIBLE View with id: class android.resources.R$id.anExampleView(1) had a visibility of View.GONE
	\end{lstlisting}}
	& 2 \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
junit.framework.AssertionFailedError:
expected: "an_example_string"
but was: ""
	\end{lstlisting}}
	& 1 \\ \hline

    \end{tabular}
\end{center}

\flaky{} tests using the same troublesome method or similar logical flow may well fail with the same stacktrace or assertion failure. By automatically detecting this, developers could target groups of related tests and potentially fix an entire group at once.


\subsubsection{Resolution}

Using data from the identification and prioritisation stages, we can begin to develop an adaptive instrumentation to automatically gather runtime information whilst maintaining a low overhead.

\todo{Explain why we are ‘just’ targeting Android for now (Shazam, GUI testing, etc).}

Automated testing is used during development. We therefore can assume unrestricted access to debug builds.


\subsubsection{Architecture}

\todo{Need a diagram to show the architecture of the Heisentest plugin. Both an abstract overview and a detailed one (implementation with Jenkins) would be useful.}

At a high level, we essentially instrument the application under test and store the gathered data. We then analyse the gathered data, adapt the instrumentation and present useful information to the developers.

To keep the scope reasonable, we currently target Java Android applications. Of course, the approach is general, and could be applied to any number of languages and frameworks. A pure Java instrumentation would be fairly similar, and actually far less complex, for reasons that will be expanded upon later.

It is necessary to give an overview of the Android pipeline in order to understand how the tool works.

In a standard Java application, code is compiled into a ‘jar’ file - an archive commonly containing compiled ‘class’ files, resources, a manifest and certificates. A java compiler will generate one class file per type (including inner and anonymous) present in the source program. At runtime, the bytecode in these classes is executed by the Java Virtual Machine (JVM).

Android applications are typically written in Java and compiled to bytecode. However, Android runs processes on Dalvik - a Virtual Machine optimised for resource-constrained devices. The compiled bytecode is then transformed to ‘dex’ format and deployed as an ‘apk’. Similar to a standard JAR file, an Android APK is simply an archive containing the compiled assets for an Android application. Crucially, the resulting dex file contains all of the classes present in the application. The format is register-based, so modifying the transformed code can be tricky.

% \todo{http://davidehringer.com/software/android/The_Dalvik_Virtual_Machine.pdf}

For the purposes of the instrumentation, many of the included files can be ignored. Figure 1. shows a simplified APK with the sections that are relevant to the tool:

\dirtree{%
.1 application.apk.
.2 META-INF\DTcomment{Directory containing certificates}.
.2 AndroidManifest.xml\DTcomment{Basic info - version, name, etc.}.
.2 classes.dex\DTcomment{Dalvik-readable compiled classes; register-based format}.
}

\subsubsection{Registers}
Frames are fixed size. This means that total registers are known ahead of time (build in to the format). Space for extra information is also included (program counter etc.) but is not of consequence to us.

Arguments of a method take the final registers of the method's invocation frame. Instance methods have a \textit{this} reference passed as the first argument; the formal arguments (if any) take up the next registers in order.

For the method:

\todo{example tiny instance method with arguments}

The corresponding registers will be allocated:

\todo{colour coded register allocation (Dalvik bytecode)}

\todo{Need to detail our register allocating visitor and explain how it works / when it (might?) fail!}

\subsection{Notes}

\subsubsection{SDK}

An SDK project can (optionally) be included. It defines the annotation @SplatterIgnore. During the isntrumentation phase, if a method is found with this annotation, it is blacklisted from the list of possible instrumentation sites. This could be useful by developers during debugging, or to remove instrumentation from sites that are known to be problematic or, alternatively, {\lq}bug-free{\rq}!

\subsubsection{Instrumentation Points}
On the first pass we look at the whole application.

\textbf{Methods}
For methods that are not blacklisted or ignored due to annotations, we store the method name and owner class (\todo{should also keep track of signature / return type?}) in our pool of {\lq}method{\rq} instrumentation points. During the second pass, these methods can be instrumented with a complex (argument-saving) or simple (counter-based) probe IF chosen during the WEKA stage.

\subsection{Logging to JSON}

Logging runs on a background thread. The Test APK drives the Application APK.

\subsubsection{Storing the results}

Android has per-application private storage, but the Gradle test runner task does not expose the \texttt{adb uninstall} task. While you can use adb shell, root and pull to grab the files from this predictable location, on Android, per-app private storage is deleted when an application is removed. Because Gradle hides the uninstall step, it is impossible to grab the Heisentest results before they are deleted.

We devised a workaround.

Android has guaranteed {\lq}public{\rq} storage (that will not be removed upon installation). This would be an obvious choice, but the location is not constant. It is likely to differ per-device and Operating System version. So, we save to this location and remember it.

When the tests finish running, we log a message to adb specifying the location of the results on the device. Then, a post-test Gradle task grabs the adb logs, parses and searches for the message to get the location. We then use adb pull to grab and finally delete the files from the device.

Issues:
If there is a huge flood of log messages, sometimes our special log message is lost in the noise (i.e., deleted before we can grab it). We partially address this by using the INFO log level --- it seems an abuse to hijack anything higher (e.g. ERROR).

\subsection{Analysis}

\todo{need to save the details of the probes we used and their associated instrumentation points so that we can infer things!}

\subsection{Hooking into testrunners}

Part of our approach is to log distinct data per-test. To keep overhead to a minimum, we take a map of test runner classes and their respective set up / tear down methods (run before / after each test in a class is run). We inject our logger set up and clean up to the setup / teardown methods respectively.

Note that during setup if the currently executing method has a @Splatter annotation attached, we look at its policy before running the logging set up. If the annotation has InstrumentationPolicy.NONE, we simply run the test as normal.

This means that within a class with multiple tests, we can disable instrumentation on any subset of the tests if the developer so desires.

It is important to note that while the @Splatter annotation with an instrumentation policy of NONE prevents the method itself from being instrumented (and in the case of a test, also prevents the logging system from being initialized), other methods will STILL potentially be instrumented. So a test with the annotation and policy that initializes an Activity and performs a sequence of actions will still have a performance penalty if called methods happen to be instrumented.

\subsection{Modifying the Behaviour}

A small project ('splatter-sdk') can be included to force certain behaviour. It is a simple library that defines a @Splatter annotation, with a parameter {\lq}InstrumentationPolicy{\rq}. When a method is annoted with this and a policy specified, Splatter will respect the defined behaviour when performing instrumentation.

Available policies are:

* NONE - Disallows all instrumentation from being injected in the method. When a test is given with this value, the logging system itself will not be initialized for that test.
* SIMPLE - Disallows complex probes from being injected in the containing method.
* COMPLEX - (Default), allows all probes to be potentially placed in the containing method.

\subsection{Logging Events}

Each event is created as a bean and held in a queue until it can be written to a JSON output file. We define an abstract LogEvent bean with common values (System Time, Thread, EventName etc.) and subclass for specific events.

\todo{We should log to a seperate blocking queue / file per thread since then we won't get blocking occuring at runtime. We can then weave the seperate log files together upon teardown.}
