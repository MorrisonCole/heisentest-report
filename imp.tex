% Focus on the interesting design decisions. For example, what were the alternatives, why select one particular solution?
% Don't flood the chapter with diagrams. Be selective.
% Avoid lengthy sections of code; use pseudo-code.
% This is a core chapter and will usually be quite substantial, 10 pages or more.
\section{Design and Implementation}
\label{sec:imp}

\begin{framed}
	\begin{itemize}
		\item Describe the design of what you have created.
		\item Start with the application architecture, giving its overall structure and the components that make up that structure.
		\item Give a description of the design of each of the components that make up the architecture.
		\item Include the database or storage representation.
		\item Provide implementation details as necessary.
	\end{itemize}
\end{framed}


\subsection{A choice of CI tools}

Probably the most well known and extensible continuous integration tool is Jenkins \cite{Jenkins}. Like other tools of its kind, it maintains a history of build results and their associated artifacts. A wealth of plugins provide support for testing frameworks such as JUnit, displaying individual test outcomes, stack traces and assertions in a simple interface.

\subsubsection{Alternatives}

Hudson, from which the Jenkins project derives is a possibility, but has a smaller user base, so has fewer plugins, fewer commits per day etc.

TeamCity \cite{TeamCity} has a significant community and a growing number of plugins, but is a closed source project maintained by JetBrains.

Although the tool will be developed initially for Jenkins, it is essential to structure the plugin such that it can easily be migrated to other CI systems. At all stages of design, this must be kept in mind.


\subsection{A Solution}

\subsubsection{Identification}

This stage is fairly trivial since we can build upon the work of others.

After a test run completes, we can gather the test run results and store them in a persistent database. We can then look at the test runs over time and attach various values to each test much in the same way as the existing tools. ‘Flakiness’ - a percentage value representing a test’s likelihood to fail will be the most useful of these.
TODO: Write a list of values we hope to attach to each test. Back this up with quotes from Shazam.

\subsubsection{Prioritisation}

We can use the data gathered during the identification stage to rank the tests according to criteria. A simple output would be a descending list ordered by flakiness:

\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    Test & Flakiness \% \\ \hline
    testVeryFlaky() & 91 \\ \hline
    testSometimesFlaky() & 33 \\ \hline
    testSignificantlyFlaky() & 2 \\ \hline
    \end{tabular}
\end{center}

Or, we could group the tests by common failure type, e.g.:

% \lstset{
% 	numbers=none,
% 	xleftmargin=3pt,
% 	numbersep=1em
% }

\begin{center}
    \begin{tabular}{| p{10cm} | l |}
    \hline
    Common Failure & Flaky Test Count \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
java.lang.OutOfMemoryError
	\end{lstlisting}}
    & 4 \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
java.lang.AssertionError:
Expected: (View visibility to be View.VISIBLE and View to have a width and a height)
but: View visibility to be View.VISIBLE View with id: class android.resources.R$id.anExampleView(1) had a visibility of View.GONE
	\end{lstlisting}}
	& 2 \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
junit.framework.AssertionFailedError:
expected: "an_example_string"
but was: ""
	\end{lstlisting}}
	& 1 \\ \hline

    \end{tabular}
\end{center}

Flaky tests using the same troublesome method or similar logical flow may well fail with the same stacktrace or assertion failure. By automatically detecting this, developers could target groups of related tests and potentially fix an entire group at once.


\subsubsection{Resolution}

Using data from the identification and prioritisation stages, we can begin to develop an adaptive instrumentation to automatically gather runtime information whilst maintaining a low overhead.

\todo{Explain why we are ‘just’ targeting Android for now (Shazam, GUI testing, etc).}

Automated testing is used during development. We therefore can assume unrestricted access to debug builds.


\subsubsection{Architecture}

\todo{Need a diagram to show the architecture of the Heisentest plugin. Both an abstract overview and a detailed one (implementation with Jenkins) would be useful.}

At a high level, we essentially instrument the application under test and store the gathered data. We then analyse the gathered data, adapt the instrumentation and present useful information to the developers.

To keep the scope reasonable, we currently target Java Android applications. Of course, the approach is general, and could be applied to any number of languages and frameworks. A pure Java instrumentation would be fairly similar, and actually far less complex, for reasons that will be expanded upon later.

It is necessary to give an overview of the Android pipeline in order to understand how the tool works.

In a standard Java application, code is compiled into a ‘jar’ file - an archive commonly containing compiled ‘class’ files, resources, a manifest and certificates. A java compiler will generate one class file per type (including inner and anonymous) present in the source program. At runtime, the bytecode in these classes is executed by the Java Virtual Machine (JVM).

Android applications are typically written in Java and compiled to bytecode. However, Android runs processes on Dalvik - a Virtual Machine optimised for resource-constrained devices. The compiled bytecode is then transformed to ‘dex’ format and deployed as an ‘apk’. Similar to a standard JAR file, an Android APK is simply an archive containing the compiled assets for an Android application. Crucially, the resulting dex file contains all of the classes present in the application. The format is register-based, so modifying the transformed code can be tricky.

% \todo{http://davidehringer.com/software/android/The_Dalvik_Virtual_Machine.pdf}

For the purposes of the instrumentation, many of the included files can be ignored. Figure 1. shows a simplified APK with the sections that are relevant to the tool:

\dirtree{%
.1 application.apk.
.2 META-INF\DTcomment{Directory containing certificates}.
.2 AndroidManifest.xml\DTcomment{Basic info - version, name, etc.}.
.2 classes.dex\DTcomment{Dalvik-readable compiled classes; register-based format}.
}

\subsection{Notes}

\subsubsection{SDK}

An SDK project can (optionally) be included. It defines the annotation @SplatterIgnore. During the isntrumentation phase, if a method is found with this annotation, it is blacklisted from the list of possible instrumentation sites. This could be useful by developers during debugging, or to remove instrumentation from sites that are known to be problematic or, alternatively, 'bug-free'!

\subsubsection{Instrumentation Points}
On the first pass we look at the whole application.

\textbf{Methods}
For methods that are not blacklisted or ignored due to annotations, we store the method name and owner class (\todo{should also keep track of signature / return type?}) in our pool of 'method' instrumentation points. During the second pass, these methods can be instrumented with a complex (argument-saving) or simple (counter-based) probe IF chosen during the WEKA stage.

\subsection{Logging to JSON}

Logging runs on a background thread. The Test APK drives the Application APK.

\subsubsection{Storing the results}

Android has per-application private storage, but the Gradle test runner task does not expose the 'adb uninstall' task. While you can use adb shell, root and pull to grab the files from this predictable location, on Android, per-app private storage is deleted when an application is removed. Because Gradle hides the uninstall step, it is impossible to grab the Heisentest results before they are deleted.

We devised a workaround.

Android has guaranteed 'public' storage (that will not be removed upon installation). This would be an obvious choice, but the location is not constant. It is likely to differ per-device and Operating System version. So, we save to this location and remember it.

When the tests finish running, we log a message to adb specifying the location of the results on the device. Then, a post-test Gradle task grabs the adb logs, parses and searches for the message to get the location. We then use adb pull to grab and finally delete the files from the device.

Issues:
If there is a huge flood of log messages, sometimes our special log message is lost in the noise (i.e., deleted before we can grab it). We partially address this by using the INFO log level --- it seems an abuse to hijack anything higher (e.g. ERROR).

\subsection{Analysis}

\todo{need to save the details of the probes we used and their associated instrumentation points so that we can infer things!}

\subsection{Hooking into testrunners}

Part of our approach is to log distinct data per-test. To keep overhead to a minimum, we take a map of test runner classes and their respective set up / tear down methods (run before / after each test in a class is run). We inject our logger set up and clean up to the setup / teardown methods respectively.

Note that during setup if the currently executing method has a @SplatterIgnore annotation attached, we skip logging altogether.

This means that within a class with multiple tests, we can disable instrumentation on any subset of the tests if the developer so desires.

It is important to note that while the @SplatterIgnore annotation prevents the method itself from being instrumented (and in the case of a test, also prevents the logging system from being initialized), other methods will STILL potentially be instrumented. So a test with the annotation that initializes an Activity and performs a sequence of actions will still have a performance penalty if called methods happen to be instrumented.

