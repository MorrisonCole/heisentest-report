% This chapter is relatively short (2-4 pages) and must leave the reader very clear on what the project is about and what your goals are.
\section{Introduction}
\label{sec:intro}

% \begin{mdframed}
% 	\begin{itemize}
% 		\item Outline the problem you are working on, why it is interesting and what the challenges are.
% 		\item List your aims and goals. An aim is something you intend to achieve (i.e., learn a new programming language and apply it in solving the problem), while a goal is something specific you expect to deliver (e.g., a working application with a particular set of features).
% 		\item Give an overview of how you carried out your project (e.g., an iterative approach).
% 		\item A brief overview of the rest of the chapters in the report (a guide to the reader of the overall structure of the report).
% 	\end{itemize}
% \end{mdframed}

\subsection{Motivation}

Software, more than ever, is ubiquitous. Deployments target the global stage, aiming to run across countless devices, Operating Systems, countries, languages and time zones. Alongside this expansion of target platforms, iteration cycle times are being ever-reduced, with teams commonly releasing new code into production environment multiple times per week. Developers have long employed automated testing to reduce risk and increase quality in the face of challenges raised by this broading scale.

Tests can be divided into three groups, each of which target an increasingly abstract layer of an application \citep[see][Chapter~16]{cohn2009succeeding}. At the lowest level, we have Unit tests, which verify the behaviour of functions through their public interfaces. Service (also known as Integration) tests target a layer above, focusing on the interactions of groups of subsystems and their responses to various inputs. Finally, at the highest level, we have User Interface (or 'Acceptance') tests, which verify application functionality as a whole system.

The further we travel up these layers, the more environmental factors we introduce. In our experience, this leads to non-determinism. Well written Unit tests are almost guaranteed to be deterministic since they run in isolation. Integration tests may depend on an external service (e.g., a database), so may sometimes see random failure. Most problematic are the Acceptance tests, which run on the complete system. The potential for unexpected behaviour here is significant. Consider an Android UI test that loads an application, selects a button and checks that a page is displayed. The test may run as expected many times, but suddenly fail with no related code changes. It could be due to anything from threading to non-standard operating system behaviour (for example, a background task could have impacted the start up time).

An automated test suite is essentially a trust mechanism. It allows changes --- functional or otherwise --- to be made with the confidence that existing behaviour will not be changed. Any unintended regressions are brought to the developers attention by one or more failing tests.

Non-deterministic tests are by definition unreliable and cannot be trusted. In our experience, within the context of real software development projects developers quickly become frustrated when dealing with non-deterministic tests and begin to rely on instinct in order to stay productive. In other words, they lose confidence in the test suite. It is essential to maintain a reliable test suite for it to remain a beneficial part of a teams' workflow.

\subsection{Shazam}

Shazam is a thriving software startup focused on audio identification with a truly global focus. As of \today, Shazam reports more than 85 million active users spanning 200 countries and 33 languages across the Android, iOS, Windows Phone and BlackBerry platforms. It is essential that the software Shazam produces is reliable, performant and to-specification. As a technology-centric company with a global focus, they employ modern development practices to ensure these requirements are able to be met constantly.

Development at Shazam is approached in a style that can be compared to the Test Driven Development described by \citet*{freeman2009growing}. Essentially, a programmer tasked with implementing a new feature will first write a failing Acceptance test to cover the feature. They then use this test to drive the specification and implementation of progressively lower level components (each thoroughly tested) until the original test passes.

Target platforms are developed by dedicated teams --- Android and iOS are the largest at \todo{ask Shazam for permission to use numbers} developers each. Of the two major platforms, Android faces the most significant fragmentation \cite{AndroidFragmentationVisualized}.

As would be expected, the Android team faces an growing number of tests as the application is developed. \todo{Ask Shazam if we can use numbers, or at least an accurate trend graph to illustrate this point.} These tests give the team the confidence to deploy the application across a huge number of devices, each differing in manufacturer, hardware, operating system, API version, form factor and screen size. Today, the Android application is covered by $N$ \todo{Shazam's permission for numbers} Acceptance level tests, as well as a host of Integration and Unit tests.

Like any real world project with a significant test suite, Shazam on Android is no stranger to non-deterministic tests. Over the period \todo{length in terms of build history} during which we collected build results from their codebase, we saw $N$ tests fail unexpectedly.

\subsection{\splatter}

We focus on developing a practical tool --- \emph{\splatter} --- to reduce the cost of non-deterministic test cases. Ultimately, each test case must be investigated and fixed by a developer. We acknowledge this and focus on gathering and analyzing information much in the same way as a developer would; autonomously and ahead of time.

First, we identify non-deterministic tests in a test suite; we call these \textit{\flaky{} tests}. Then, we employ techniques similar to those discussed by \citet{ArumugaNainar:2010:ABI:1806799.1806839} to gather relevant debugging information over a series of test runs. Finally we analyze the gathered data and identify predicates strongly associated with test failure. Our tool takes advantage of the continuous integration environment, always running and constantly improving its models.

One of the primary initial goals of this paper was to create a continuous integration plugin to present the data gathered during the identification of the \flaky{} tests. As we researched the area, we realised that the indentification of \flaky{} tests is the most well-covered and least interesting area. The more experimental and relatively untouched area dealing with adaptive instrumentation and analysis of collected information eventually became our primary focus.

Reaching a stage where we could analyze the gathered data proved to be far more challenging than expected. The relative youth of the Android platform was a clear stumbling block, since tried and tested instrumentation tools in the world of the Java Virtual Machine had only just begun to acknowledge their Dalvik sibling. As such, we document and discuss many of our design decisions along the way.

\todo{Walk the reader through the upcoming sections of the paper. Can only really do this at the end!}