\section{Approach}
\label{sec:approach}


\subsection{Fixing flaky tests}

\todo{Have a comparison between the results gathered normally (logs + assertion failure/stacktrace + screenshot) and heisentest (JSON logs).There are strong ties to ‘heisenbugs’ and ‘debuggability’ here. Perhaps get references.}

Sometimes, it is not possible to fix a flaky test from the information output by the testrunner alone. Typically, the information includes logs, an assertion failure message and/or a stack trace. Applications with a user interface may also provide a number of screenshots. Often, a developer will have to manually gather information to build up an understanding of the intended flow vs. the actual flow on failing runs. For flaky tests with a low failure rate, this can be a painstaking and time-consuming process. Indeed, if the flaky test is a timing based issue, attaching a debugger may cause the test to pass indefinitely.

\todo{Have a comparison between the results gathered normally (logs + assertion failure/stacktrace + screenshot) and heisentest (JSON logs).}


\subsection{Aims and Goals}

Flaky tests are hard to fix. In general, there are three stages during which we may intervene:
\begin{enumerate}
	\item Identification - which tests in the suite are flaky?
	\item Prioritisation - why of the flaky tests should we fix first?
	\item Resolution - what information can we gather to speed the resolution of a test?
\end{enumerate}

Ultimately, the final stage is the target. In order to tackle the problem practically, knowledge gathered at each stage must be combined and considered.

Existing tools (Shazam’s Flaky Test Monitor included) provide answers to 1. This information can be used to inform manual inspection of 2. As of yet, no tool exists to tackle 3.

In a typical software development project, continuous integration systems will be set up from the beginning. Developers write code and commit to source control. A master server will detect the change and assign one of a number of build agents (other servers with the capability to build the project(s) - virtual or otherwise) an attached job.

For a typical job, the chosen build agent pulls down the latest changes, compiles and run the tests and runs any post-build tasks. At the end, any build artifacts (distributable packages, test results, etc.) will be uploaded to the master server and the agent will be once again free to build the next iteration. Note that in reality, a job can comprise of any number of runnable steps - from executing a shell script to hitting an external server.

\todo{A diagram of this flow would be nice.}

It is obvious that, if we are to develop a tool to assist developers with flaky tests, we should run as part of a modular continuous integration system.


\subsection{Formal Definition of a Flaky Test}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

% Noticed that 'A Hueristic Test Data Generation Approach for Program Fault Localization' has a nice definition for passing/failing test cases (no notion of flakiness though). See section 3.1, Definition 1.

\begin{defn}
	Given a subject under test $f: \vec{I} -> \vec{O}$.

	A test case is $(\vec{i},\vec{o})$

	The “fixed f” condition needs fixing. It will need to be wrt the slice from the program point at which $\vec{o}$ is generated to entry, given $\vec{i}$. In others words, it’s impact analysis. Intuitively, a test can only be flaky when its behavior is sensitive unknown inputs and not to changes to f that it, in fact, is designed to test.

	A \emph{flaky test case} is a test case where, for fixed $f$,
	$p(f(\vec{i} = \vec(o)) = 1 - \epsilon$

	Remark: A flaky test case is an unfair coin.

	Let $\vec{I}$ be the lifting of all inputs, including coin flips and environmental interactions, into a single input vector.

	The key is the true $\vec{I}$ is only partially known;  we capture flakiness as unknown components of $\vec{I}$, like Todd Mytkowicz’ sensitivity to the length of environmental variables, etc.

	Note: formalize how much data we will need to gather in order to discover the cause of flakiness as a function of epsilon. Rare events, like flakiness, are related to smoothing.

	Combined with our budget b, we can determine what values of $\epsilon$ we can afford to detect!
\end{defn}

Test suite $f$ with flaky tests $f!$.
Budget $B_{f} = B_{6} + B_{f!} + B_{nd}$

\subsection{Flakiness}

\begin{quote}
	Test failure can be modelled probailistically. A flaky test is one known to fail with low probablitiy.
\end{quote}

\begin{defn}[Flaky Test]\label{def:flakytest}



\end{defn}

Each test is assigned a value in the range $[0-1]$, representing its flakiness. The higher the flakiness, the less likely the test is to fail on any given run. This value is used during test suite budget allocation.

\begin{figure}[here]\label{fig:app:flakiness}
\caption{}
	\begin{center}
		% See: http://www.texample.net/tikz/examples/line-plot-example/
		\begin{tikzpicture}[thick, framed, x=6.5cm, y=1.6cm]
			% Title
			\draw (0.5, 1) node[above] {$\textsc{Flaky Tests}$};

			% Axis
			\draw (0,0) -- coordinate (x axis mid) (1,0);
		    \draw (0,0) -- coordinate (y axis mid) (0,1);

		    % Axis Labels
		    \node (padding) [below] at (0.4, 0) {};
			\node[below of = padding] at (x axis mid) {Chance of Failure};

		    % Ticks
		    \foreach \x in {0, 0.4, 1}
		    	\draw (\x, 1pt) -- (\x, -3pt) node[anchor=north] {\x};
		    \foreach \y in {0, 1}
		    	\draw (1pt, \y) -- (-3pt, \y) node[anchor=east] {\y};

			% Flaky test range
		    \draw[<-] (0, 0.5) -- (0.4, 0.5); % Draw horizontal line
		    \draw (0.4, 0.42) -- (0.4, 0.58); % Draw right vertical tab

		    % Flakiness label
		    \node [above] at (0.2, 0.5) {$Flaky$};

		    % Alpha label
		    \node [right] at (0.4, 0.5) {$(\boldsymbol{\alpha})$};
		\end{tikzpicture}
	\end{center}
\end{figure}

Figure~\ref{fig:app:flakiness} shows the failure rates that qualify as 'flaky'. If a tests' chance of failure lies within the range $(0, \alpha]$, its flakiness value will be $(0, 1]$; otherwise, it will be $0$.

\todo{What value is $\alpha$, how do we calculate it? 0.4 is arbitrary!}
\todo{We ignore tests with failure rates greater than $\alpha$ since they could be reasonably reproduced by a developer?}

\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}

\begin{algorithm}[here]\label{alg:app:calculateFlakiness}
\caption{Calculating the flakiness of a test}
\begin{algorithmic}
	\Require{$chanceOfFailure$ and $threshold$ are decimal values within ranges $[0, 1]$ and $(0,1]$ respectively.}
    \Statex

	\Function{calculateFlakiness}{$chanceOfFailure$, $threshold$}

	\If{$chanceOfFailure \geq threshold$ OR $chanceOfFailure \eq 0$}
		\State \Return $0$
	\Else
		\State \Return $1 - (chanceOfFailure / threshold)$
	\EndIf

	\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Determining Test Suite Budget}

There are two factors to take into account when determining our overall budget:

\begin{description}
	\item[Total time available] \hfill \\
 		Our upper bound for run time. This can be specified by the developer(s), or left as a sensible default.
	\item[Average run time of each test] \hfill \\
		Instrumentation comes with overhead. If we know the average run time of a suite without instrumentation and the estimated overhead of probes, we can apply instrumentation until we hit our upper bound. If we end up running over in practice, we simply reduce the budget and vice versa.
\end{description}

\subsection{Allocating Test Suite Budget}

Test suite budget is distributed across all tests. We split our budget into two pools --- targeted and exploratory. We allocate targeted budget to tests with high flakiness, prioritising those with the highest flakiness and least historical data. Exploratory budget is randomly allocated to the remaining tests in the hopes that we get lucky and catch a previously stable test fail unexpectedly.

\todo{Consider NEW tests in the test suite. Should we instrument them right away or wait until we've seen them run without instrumentation $n$ times, etc.?}


% The {algorithm} wrapper is pretty unattractive as far as I'm concerned.
% Need to look into alternative ways of formatting this.
\begin{algorithm}[h]
\caption{Instrumenting the test suite with respect to a budget}
\label{alg:splatter}
\begin{algorithmic}
	\Require{$tests$ is an set comprising all the tests in the test suite}
	\Require{$proportionTargeted$ is a decimal value in the range $[0, 1]$}
	\Statex

	\Function{splatter}{$tests$, $proportionTargeted$}

	\State $suiteBudget \gets$ calculateSuiteBudget($tests$)
	\Statex

	\State allocateBudget($tests$, $suiteBudget$, $proportionTargeted$)
	\Statex

	\ForAll {$test \in tests$}
		\State{instrument($test$)}
	\EndFor

	\EndFunction

	\Statex

	\Function{allocateBudget}{$tests$, $suiteBudget$, $proportionTargeted$}

	\State{$targetedBudget \gets suiteBudget * proportionTargeted$}
	\State{$exploratoryBudget \gets suiteBudget * (1 - proportionTargeted)$}
	\Statex

	\State{allocateTargetedBudget($tests$, $targetedBudget$)}
	\State{allocateExploratoryBudget($tests$, $exploratoryBudget$)}

	\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:splatter} defines the two main steps: calculation and instrumentation. First a per-test budget is calculated based on our knowledge base; finally the instrumentation is performed. It is during the second step that the application is actually modified.


\subsubsection{Targeted Budget Allocation}

\begin{algorithm}[H]
\caption{Allocating targeted budget to tests}
\label{alg:allocateTargetedBudget}
\begin{algorithmic}
	\Function{AllocateTargetedBudget}{$tests$, $targetedBudget$}

	\State $totalFlakiness \gets 0$

	\ForAll{$test \in tests$}
		\State $totalFlakiness \gets totalFlakiness + test.flakiness$
	\EndFor
	\Statex

	\State $tests \gets \text{sortByFlakinessDescending(}tests$)

	\ForAll{$test \in tests$}
		\State $test.budget \gets$ calculateTestTargetedBudget($test$, $totalFlakiness$, $targetedBudget$)
	\EndFor

	\EndFunction
	\Statex
	\Function{calculateTestTargetedBudget}{$test$, $targetedBudget$}

	\State $maximumTargetedBudget \gets \todo{someNumberThatWeCalculate}$

	\State $budget \gets (test.flakiness / totalFlakiness) * targetedBudget$
	\Statex

	\Return minimum($testTargetedBudget$, $maximumTargetedBudget$)

	\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Exploratory Budget Allocation}

\begin{algorithm}[h]
\caption{Allocating exploratory budget to tests}
\label{alg:allocateExploratoryBudget}
\begin{algorithmic}[1]
	\Function{AllocateExploratoryBudget}{ $tests$, $B_e$}

	\State shuffle($tests$)
	\State $iterator \gets tests.getIterator()$
	\While{$exploratoryBudget > 0$ AND $iterator.hasNext()$}
		\State $currentTest \gets tests.getNext()$
		\State \begin{varwidth}[t]{\linewidth}
		$exploratoryBudget \gets exploratoryBudget -$\par
		\hskip\algorithmicindent \text{calculateTestExploratoryBudget}(test, exploratoryBudget)
		\end{varwidth}
	\EndWhile

	\EndFunction
	\Statex
	\Function{CalculateTestExploratoryBudget}{test, exploratoryBudget}

	\State $maximumExploratoryBudget \gets \todo{someNumberThatWeCalculate}$

	\If{$test.budget > 0$}
		\Return $0$
	\EndIf

	\State $test.budget \gets$ minimum($exploratoryBudget$, $maximumExploratoryBudget$)

	\State \Return $test.budget$

	\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Instrumentation}

\todo{Pseudocode should show instrumentation of an individual test based on previous instrumentation sites, budget, etc.}

\begin{algorithm}[H]
\caption{Instrument a test with respect its allocated budget}
\label{alg:instrument}
\begin{algorithmic}
	\State{\textbf{instrument} (test)}
	\State{$sites \gets test.instrumentationSites$}
	\While{$test.budget > 0$}
		\ForAll{$site \in sites$}
			\State{$cost \gets site.cost$}
			\If{$cost \le test.budget$}
				\State{$site.active \gets true$}
				\State{$test.budget \gets budget - cost$}
			\Else
				% Need to work out how to define new algorithmic-style macros (e.g. \BREAK)
				\State{\textbf{break}}
			\EndIf
		\EndFor
	\EndWhile
\end{algorithmic}
\end{algorithm}


\subsection{Monitoring Application State with Probes}

There are a number of types of points at which it is useful to gather information about application state. We extend the common approach of counters and predicates[] to include instrumentation designed for to gather much more contextual information.

We refer to a piece of code inserted at an instrumentation site to monitor application state as a probe. Complex probes record the state of live objects such as variables and method parameters, but carry a large performance overhead. Predicate probes are a lightweight alternative to monitor execution flow.

Probes are allocated based on the available budget, historical results and available instrumentation sites. For example, predicate probes could be used to hone in on program flows associated with failure during initial runs. Later, once areas of problematic code are identified, heavier-weight complex probes could then be inserted to gather more detailed information.

\todo{Dropped the ‘scalar value monitoring’ of previous approaches, since we can monitor a lot more with our budget when we are sure we’ve found an area of problematic code.}
\todo{Optimal placement of probes is NP-Hard?}

Complex probes serialize java objects to JSON with google-gson. Primitives are boxed to their object representations.

Predicate probes simply maintain a counter that is incremented each time the predicate is observed to be true.

\begin{center}
    \begin{tabular}{| l | p{6cm} | l |}
    \hline
        \textbf{Program Point} & \textbf{Description} & \textbf{Cost} \\
    \hline
        \multicolumn{3}{|c|}{\textit{Complex Probes}} \\
    \hline
        Function Entry &
        At the beginning of each function, we serialize all parameter objects (boxing primitives), along with all live variables. &
        \todo{Cost} \\
    \hline
        Function Return &
        At each function call with a return value, we serialize its object result. &
        \todo{Cost} \\
    \hline
        Conditional Branch &
        At each branch point, we record all variables in scope separately for each path. &
        \todo{Cost} \\
    \hline
        Variable Assignment &
        For an assignment $V = E$, where $E$ is an expression, we record both the current value of $V$ and the new value of $V$ after assignment. &
        \todo{Cost} \\
    \hline
        \multicolumn{3}{|c|}{\textit{Predicate Probes}} \\
    \hline
        Conditional Branch &
        At each path of the branch we maintain a counter that is incremented every time the path is taken. &
        \todo{Cost} \\
    \hline

    \end{tabular}
\end{center}


\subsection{Choosing Instrumentation Sites}

There are potentially millions of lines of code, where do we place our instrumentation?

Given an entry point, in our case, a test or setup method, we simply take the next set of code units in the control flow graph.

The main strategy is to place probes further and further down the control dependence graph until we are able to detect a failure-predicting predicate.

Once we have detected a failure-predicting predicate, we can assign the majority of our budget to drill down in related areas. Still, we reserve a portion to ‘scout’ in case we get lucky and find another failure predicting predicate elsewhere.


\subsection{Differences between previous approaches and ours}

Liblit’s Adaptive Bug Isolation [] and other papers have taken a conservative approach to information gathering. The projects have targeted production code, so privacy and performance are major concerns.

\todo{Citations - there are a lot to add here!}


\subsubsection{Ordering}

In previous statistical bug isolation projects, ordering is completely discarded due to privacy concerns []. Recording a play-by-play execution is invasive to the common user.

Since our instrumentation will run in a development environment, there are no user concerns - the tests are automated. We can maintain ordering with a little more overhead.

Multi-threaded environments are commonplace. In order to record the execution order of multiple threads, we include the system time in each log event. Each thread logs to a separate sink. After the tests run completes, we merge and interleave the individual logs before storing them. We end up with a single log file with times and thread IDs.


\subsubsection{Storage/Result Collection}

Again, the context of our execution allows more flexibility. In production systems, logs have to be stored on user devices and (eventually) transferred to a central location for analysis. User storage space and bandwidth is precious, so it is essential to minimise both.

In our case, tests will be run internally on project-owned machines and devices. Log files can be transferred to the central database immediately following a test run. Each test run by definition requires a clean device, so build agents will almost certainly never run out of space since they will at any moment be storing the logs from at most one test run.

The only real storage concern is that of the central database. But, this can be managed effectively by limiting the number of historical test run logs to keep - much in the same way Jenkins and other CI tools do by default.


\subsubsection{Performance}

Instrumentation adds performance overhead. In the case of a production system, this is a major problem since performance directly affects a user’s experience. Nainar and Liblit \cite{ArumugaNainar:2010:ABI:1806799.1806839} propose an adaptive bug isolation system with a performance overhead of just 1\%.

In a test environment, smoothness and load times rarely matter. Of course, there are exceptions (performance regression tests, etc.), but we expect to mainly be dealing with system tests. We can safely add instrumentation and ignore performance, unless it begins to affect the thread-wise execution. If a flaky test begins consistently passing when heavily instrumented, we can simply reduce the instrumentation until the previous flaky behaviour is once again observed.

\subsubsection{Adaptivity}

Both fixed and adaptive approaches have been proposed[] in the past. All of these approaches were developed with the underlying constraint of deploying the instrumented software to real users. [adaptive bug isolation] makes use of binary instrumentation to iteratively re-instrument deployed applications to hone in on a bug-predicting predicates. Whilst the adaptive approach has many benefits in terms of overhead, it relies on a specialized API - Dyninst - for code patching to support the injection of  instrumentation at runtime. This has additional runtime costs \cite{DyninstGuide} associated with saving and restoring registers and performing protective checks not present in a fixed instrumentation.

Again, our context allows more freedom. Every run requires a new build by nature, so simply apply a unique fixed instrumentation every time. In other words, we retain the optimisation benefits of a fixed instrumentation whilst gaining those of the adaptive solution.
