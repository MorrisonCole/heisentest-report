\section{Approach}
\label{sec:approach}


\subsection{Fixing flaky tests}

\todo{Have a comparison between the results gathered normally (logs + assertion failure/stacktrace + screenshot) and heisentest (JSON logs).There are strong ties to ‘heisenbugs’ and ‘debuggability’ here. Perhaps get references.}

Sometimes, it is not possible to fix a flaky test from the information output by the testrunner alone. Typically, the information includes logs, an assertion failure message and/or a stack trace. Applications with a user interface may also provide a number of screenshots. Often, a developer will have to manually gather information to build up an understanding of the intended flow vs. the actual flow on failing runs. For flaky tests with a low failure rate, this can be a painstaking and time-consuming process. Indeed, if the flaky test is a timing based issue, attaching a debugger may cause the test to pass indefinitely.

\todo{Have a comparison between the results gathered normally (logs + assertion failure/stacktrace + screenshot) and heisentest (JSON logs).}


\subsection{Aims and Goals}

Flaky tests are hard to fix. In general, there are three stages during which we may intervene:
\begin{enumerate}
	\item Identification - which tests in the suite are flaky?
	\item Prioritisation - why of the flaky tests should we fix first?
	\item Resolution - what information can we gather to speed the resolution of a test?
\end{enumerate}

Ultimately, the final stage is the target. In order to tackle the problem practically, knowledge gathered at each stage must be combined and considered.

Existing tools (Shazam’s Flaky Test Monitor included) provide answers to 1. This information can be used to inform manual inspection of 2. As of yet, no tool exists to tackle 3.

In a typical software development project, continuous integration systems will be set up from the beginning. Developers write code and commit to source control. A master server will detect the change and assign one of a number of build agents (other servers with the capability to build the project(s) - virtual or otherwise) an attached job.

For a typical job, the chosen build agent pulls down the latest changes, compiles and run the tests and runs any post-build tasks. At the end, any build artifacts (distributable packages, test results, etc.) will be uploaded to the master server and the agent will be once again free to build the next iteration. Note that in reality, a job can comprise of any number of runnable steps - from executing a shell script to hitting an external server.

\todo{A diagram of this flow would be nice.}

It is obvious that, if we are to develop a tool to assist developers with flaky tests, we should run as part of a modular continuous integration system.


\subsection{Formal Definition of a Flaky Test}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

% Noticed that 'A Hueristic Test Data Generation Approach for Program Fault Localization' has a nice definition for passing/failing test cases (no notion of flakiness though). See section 3.1, Definition 1.

\begin{defn}
	Given a subject under test $f: \vec{I} -> \vec{O}$.

	A test case is $(\vec{i},\vec{o})$

	The “fixed f” condition needs fixing.  It will need to be wrt the slice from the program point at which $\vec{o}$ is generated to entry, given $\vec{i}$. In others words, it’s impact analysis.  Intuitively, a test can only be flaky when its behavior is sensitive unknown inputs and not to changes to f that it, in fact, is designed to test.

	A \emph{flaky test case} is a test case where, for fixed $f$,
	$p(f(\vec{i} = \vec(o)) = 1 - \epsilon$

	Remark: A flaky test case is an unfair coin.

	Let $\vec{I}$ be the lifting of all inputs, including coin flips and environmental interactions, into a single input vector.

	The key is the true $\vec{I}$ is only partially known;  we capture flakiness as unknown components of $\vec{I}$, like Todd Mytkowicz’ sensitivity to the length of environmental variables, etc.

	Note:  formalize how much data we will need to gather in order to discover the cause of flakiness as a function of epsilon.  Rare events, like flakiness, are related to smoothing.

	Combined with our budget b, we can determine what values of $\epsilon$ we can afford to detect!
\end{defn}

% See: http://www.texample.net/tikz/examples/line-plot-example/
\begin{tikzpicture}[thick, framed, x=6.5cm, y=1.6cm]
	% Title
	\draw (0.5, 1) node[above] {$\textsc{Flaky Tests}$};

	% Axis
	\draw (0,0) -- coordinate (x axis mid) (1,0);
    \draw (0,0) -- coordinate (y axis mid) (0,1);

    % Axis Labels
    \node (padding) [below] at (0.4, 0) {};
	\node[below of = padding] at (x axis mid) {$Flakiness$};

    % Ticks
    \foreach \x in {0, 0.4, 1}
    	\draw (\x, 1pt) -- (\x, -3pt) node[anchor=north] {\x};
    \foreach \y in {0, 1}
    	\draw (1pt, \y) -- (-3pt, \y) node[anchor=east] {\y};

	% Flaky test range
    \draw[<-] (0, 0.5) -- (0.4, 0.5); % Draw horizontal line
    \draw (0.4, 0.42) -- (0.4, 0.58); % Draw right vertical tab

    % Alpha label
    \node [right] at (0.4, 0.5) {$(\boldsymbol{\alpha})$};
\end{tikzpicture}


Test suite $f$ with flaky tests $f!$.
Budget $B_{f} = B_{6} + B_{f!} + B_{nd}$

\paragraph{Instrumentation pseudocode:}

% Use 'in' for-loop range notation rather than 'to'.
\renewcommand{\algorithmicto}{\textbf{in}}

% The {algorithm} wrapper is pretty unattractive as far as I'm concerned.
% Need to look into alternative ways of formatting this.
\begin{algorithm}
\caption{Allocate instrumentation budget across a test suite}
\label{alg1}
\begin{algorithmic}
	% Perhaps this should just take a single tests[]. We can query the element to determine its 'priority' (relative flakiness).
	% Or, could take a vector<test, flakiness> to be explicit.
	\STATE{\textbf{splatter} (budget, flakyTests[], allTests[])}
	\STATE{}
	\COMMENT{Need a function that looks at all the tests and their associated priorities, orders them and attaches an allowedBudget value in terms of the whole budget}
 	\WHILE{$budget \geq 0$}
 		% Simpler if the 'instrumentTest' function returns a value representing the amount of budget it actually used, rather than its remainder.
 		\STATE{$budget \gets budget - allowedBudget + \textbf{instrumentTest} (test, allowedBudget)$}
	\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Instrument a test with respect to a given budget}
\label{alg2}
\begin{algorithmic}
	\STATE{\textbf{instrumentTest} (test, budget)}
	% Should 'sites' just be a parameter?
	\STATE{$sites \gets test.instrumentationSites$}
	\FOR{$site$ \TO $sites$}
		% Should probably define a cost function (e.g. cost(instrumentationPoint)) and use that, rather than using an unexplained accessor.
		\STATE{$cost \gets site.cost$}
		\IF{$cost \le budget$}
			\STATE{$site.active \gets true$}
			\STATE{$budget \gets budget - cost$}
		\ELSE
			% Need to work out how to define new algorithmic-style macros (e.g. \BREAK)
			\STATE{\textbf{break}}
		\ENDIF
	\ENDFOR
	\RETURN{budget}
\end{algorithmic}
\end{algorithm}
