\section{Conclusions and Future Work}
\label{sec:conc}

Our aim is a framework that speeds the resolution of \flaky tests in a test
suite. We propose an ideal system where tests are instrumented with respect to a
budget. We produce proof of concept named for the Android platform that
integrates with Jenkins with a plugin. Our proof of concept is shown to
successfully instrument an Android application by inserting probes at
instrumentation points with respect to certain conditions. We present and
analyse initial cost measurements for two of the implemented probe types.

Our proof of concept, \venera, does not implement any of the budget logic laid
out in our approach. The tool was designed with budget in mind, so future work
building up this functionality is possible.

We initially spread our efforts across the indentification, prioritisation and
resolution areas. As the project progressed, the resolution stage gradually
became our sole focus. Although this was unfortunate for our industry partner,
without this shift the scope would have been too large for the given time scale.
In fact, the engineering and theory surrounding resolution alone could be
developed far further than we have already taken it.


\subsection{Future Work}
\label{sec:sec:future_work}

We have assumed that flaky tests are costly to practitioners that encounter them
in their projects. To determine whether this is indeed the case within the
context of at least one real-world software development project, we plan to do a
case study with the developers at Shazam. We hope to gain an understanding of
the ways developers deal with \flaky tests, what tools they use and how they
feel \flaky tests affect the team.

In order to support the development of the tool, we plan to deploy \venera to
the Android team at Shazam in the near future. Seeing how it behaves and how
developers interact with it will be critical in steering the engineering work.

Finally, we also plan to evaluate the performance of \venera. For some known
\flaky tests that have been fixed, we will apply \venera and analyse its
performance. We will measure probe proximity to the source of the \flaky test
over a series of runs. Presuming \venera works as expected, we would hope to
see probes placed in locations closer to the bug as a test is run repeatedly.

In order to be helpful in practice, \venera would ideally reduce the expected
time to fix a flaky test for some or all types of developer (experienced,
inexperienced, \etc.). We would need to compare a set of developers using the
tool to a set of developers without the tool in experimental conditions. The
developer sets could either be of mixed or similar experience, and working on a
familiar or unfamiliar codebase. Familiarity of the group using \venera with
the tool will also need to be accounted for.


\subsection{Future Design}
\label{sec:sec:future_design}

\venera has been a successful proof of concept so far, but there are many areas
in which it can be improved. The following sections detail some of the ways that
it may be pushed in the future.

\subsubsection{Probe Placement}

We can improve our probe placement strategy by taking into account control flow
graphs and loops.

\subsubsection{Prioritisation}

In the future, we could adjust \flaky budget allocation by taking into account
relative volume of historical data. For example, a newly \flaky test for which
we have little to no data may be prioritised above a known \flaky test for which
we hold a large amount of information from previous instrumented runs.

We could start from a pesimistic viewpoint. A new tests would be assumed to have
a flakiness value of 1. As the test is run, the flakiness value would be
adjusted as usual until it represented a more accurate value. New tests will
therefore be heavily instrumented, and gradually less so as they prove
themselves to be reliable.

The \jenkinsPlugin could be improved as far as prioritisation is concerned.
We could use data gathered during the identification stage to rank the tests
according to criteria; a simple output would be a descending list ordered by
flakiness. Or, we could group the tests by common failure type, \eg:
\begin{center}
    \begin{tabular}{| p{10cm} | l |}
    \hline
    Common Failure & \flaky Test Count \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
java.lang.OutOfMemoryError
	\end{lstlisting}}
    & 4 \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
java.lang.AssertionError:
Expected: (View visibility to be View.VISIBLE and View to have a width and a height)
but: View visibility to be View.VISIBLE View with id: class android.resources.R$id.anExampleView(1) had a visibility of View.GONE
	\end{lstlisting}}
	& 2 \\ \hline
    {\begin{lstlisting}[language=Java, numbers=none]
junit.framework.AssertionFailedError:
expected: "an_example_string"
but was: ""
	\end{lstlisting}}
	& 1 \\ \hline

    \end{tabular}
\end{center}

\Flaky tests using the same troublesome method or similar logical flow may well
fail with the same stacktrace or assertion failure. By automatically detecting
this, developers could target groups of related tests and potentially fix an
entire group at once.

\subsubsection{Gathering Data from Local Test Runs}

In a typical test driven development cycle, tests are run constantly --- both by
the developers and the continuous integration build agents. We could gather test
run data from the tests run by developers themselves by sending the results over
the network to the \jenkinsPlugin.

\begin{itemize}
	\item This would require {\lq}noise{\rq} reduction --- developers break tests
	locally often, we wouldn't want to adjust our flakiness values based on
	changes that never make it onto the development branch.
	\item It should be easy to {\lq}opt-out{\rq} of this --- our instrumentation
	does affect build times and developers may not appreciate the extra overhead!
\end{itemize}

\subsubsection{Commit Blaming}

Our CI plugin could display an estimated commit {\lq}blame{\rq} range, along
with relevant build numbers if a previously stable test is suddenly observed to
be \flaky, and vice versa.

\subsubsection{Retiring Trusted Tests}

Projects with a huge number of tests face lengthy build times. It may be in a
teams' interest to remove trusted tests from the per-commit build and instead
run them on a seperate cycle to tighten the immediate feedback loop. We could
provide a listing of {\lq}most trusted{\rq} tests (\ie, tests that have
exhibited no flakiness over a significant period).

\subsubsection{Porting the Plugin}

Jenkins is but one of many available continuous integration tools
\cite{ContinuousIntegrationSoftware}. Hudson, TeamCity, CruiseControl and Team
Foundation Server are amongst the notable alternatives. A Hudson port would be
especially trivial, since the Jenkins codebase is still fairly similar. In fact,
the code-bases have not diverged significantly since 2010 when Jenkins
materialised.

TeamCity \cite{TeamCity} has a significant community and a growing number of
plugins, but is a closed source project maintained by JetBrains.

\subsubsection{Logging Performance}

We could log to a seperate blocking queue and file per application thread. After
the test run is complete, a simple script could {\lq}weave{\rq} the files back
together using thread IDs and system times. This would reduce runtime blocking.
However, we should measure performance in more detail to identify the real
bottlenecks before applying theoretical performance improvements.

\subsubsection{Continuous Integration}

In the future, we could look at the test runs over time and attach various
values to each test much in the same way as the existing Continuous Integration
tools. It will also need to be extended with an interface to visualise test
flakiness and related information.
