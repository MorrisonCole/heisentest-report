% This chapter should cover background information, related work, research done and tools or software selected for use in the project.
% You should not include well-known things (e.g., HTML or Java) or try to give tutorials on how to use a tool or code library (use references to books and websites for that information). Everything you include should be directly relevant to your work and the relationship made clear.
% This chapter is likely to be fairly substantial, perhaps 8-10 pages.
\section{Context}
\label{sec:context}

\begin{framed}
	\begin{itemize}
		\item Provide the necessary context and background information to describe how your project relates to what is already known or available.
		\item If relevant, a survey of similiar solutions, programs or applications to yours, and how yours is differentiated.
		\item A description of the research carried out to learn about the nature of the problem(s) being investigated and potential solutions. The form of the research will vary widely depending on the kind of project. For example, it might involve searching through research publications and online resources, or might involve an exploration of design possibilities for a user interface or program structure.
		\item Outline and reference the sources of information you are drawing on (papers, books, websites, etc.). State how each relates to your work.
		\item Introduce the software, programming languages, library code, frameworks and other tools that you are using. Discuss choices and make clear which you made use of and why.
	\end{itemize}
\end{framed}

\subsection{Test Driven Development}

Test driven development, when taken to its limits, works like this:

\begin{itemize}
	\item A developer receives a specification for a feature (this could be a user story, given when thens i.e. BDD-style - cucumber, or any number of different presentations)

	\item The developer writes a system test to ensure the feature works. System tests operate at the system level, so they often read like BDD requirements, e.g.:

	\begin{verbatim}
  	Scenario: Add to Basket
  	  Given I am viewing the store page
  	    And I have no items in my basket
  	  When I add an item to my basket
  	  Then that item is shown in my basket
	\end{verbatim}

	Could be eventually converted in to the following Java code:

	\lstinputlisting[language=Java]{bddExample.java}

	\item The developer runs/edits the test until it fails for the right reason.

	\item The developer begins implementing the feature - again writing tests first, but this time on the finer levels of unit / integration.

	\item When all tests pass, the feature is completed.
\end{itemize}

Note that there are many points for interpretation along the steps of this process. For example, an interface may be tested for the presence of a specific image, but its location may be ignored. Because of this, manual spot checks and alpha / beta staged releases are often used.

In order for a test suite to improve a team’s work, it must be trustworthy. Tests must be written carefully to ensure that they fail when the code under test is indeed broken, and pass otherwise. As is often the case, when tests fail unexpectedly, they are removed from the suite until they are fixed.

Unfortunately, tests running at the system level are prone to intermittent failure.

When a subset of tests fail randomly each time the suite is run, it is all too easy for developers to begin ignoring real failures if and when they do occur.

\subsection{Why are Flaky Tests a problem?}

Consider a project with a team of 5 developers. They write system, integration and unit tests to cover every feature they add to the software. The current test suite $T$, made up of tests $t_{0}, t_{1}, \dots, t_{n}$, takes 20 minutes to run and each developer is expected to run the entire suite locally before pushing a new feature to the development branch. The test suite contains a number of flaky system tests $F$ written $f_{0}, f_{1}, \dots, f_{k} \text{, where $k \leq n$}$ that the team are aware of. $F \subseteq T$.

A developer is working on a new feature and has written a system test to ensure it works as expected. The system test passes, so the developer prepares to push the code onto the development branch. Before doing so, the developer must run the test suite locally to ensure the work did not break any existing functionality. The test suite runs, but 4 of the system tests --- $t_{3}, t_{7}, t_{9} \text{ and } t_{21}$ --- fail.

The developer is sure they’ve seen $t_{3}, t_{7} \text{ and } t_{9}$ fail randomly before, i.e. $t_{3}, t_{7}, t_{9} \in F$. After checking with colleagues, they assume that $t_{21} \in F$, since it appears to be unrelated to the area of code they’ve been working on. The developer pushes the code.

In this example, $t_{21}$ was not actually flaky, but had been broken by the developers changes. It is important to recognise that, whilst in some situations it may be possible to run the test suite multiple times to be sure, sometimes this is impractical. Test suites may take hours, days or even months to run in their entirety, and may contain many flaky tests.

After the code is pushed, a different set of tests fail on the build server: $t_{3}, t_{7} \text{ and } t_{9}$ are now passing, but $t_{21}$ has once again failed. In this case, it is obvious that $t_{21}$ must be investigated, so the developer’s changeset is rolled back whilst they fix the change.

This may appear to be workable, but time was almost certainly wasted. The developer writing the new feature only became aware that they had broken existing functionality after the second run revealed a trend. If $t_{21}$ had failed alone, the developer would have taken action before committing the changeset.

\subsection{Why is committing a broken changeset bad?}

Any commits made on top of the broken change may need to be reviewed.

\subsection{Living with Flaky Tests}

When the latest changes are due to be released, the flaky tests must be considered. Normally, a particular software iteration would be held back until all tests pass, but with flaky tests thrown into the mix - it may take multiple runs before all the tests pass at once - again, adding overhead.

The set of flaky tests cause confusion, obscure real failures and erode trust.

\subsection{Why not just ‘suppress’ the flaky tests?}

Informally, let a valid test be a test that asserts that some unit of the application behaves as expected. A redundant test might check behaviour that is not required, or is tested thoroughly elsewhere. Assuming all tests in a suite are valid, then any flaky tests in the suite are also valid. A flaky test may cover behaviour that is not tested elsewhere. Suppressing (temporarily retiring) the flaky test may do more harm than good, since the behaviour it previously tested will now be go completely unchecked unless manually tested upon each commit.

\subsection{Existing Tools}

There are continuous integration tools that aim to mitigate flaky test impact. Namely, the Chromium Flakiness Dashboard, Shazam’s internal ‘Flaky Test Monitor’ and various Jenkins plugins. Typically, these attach a ‘flakiness’ value to each test case that represents the likelihood of failure during any given test run. Some also expose statistics such as stability (the tests flakiness across the previous n runs), successful runs since last failure and first known failure. Both the Shazam Android and Chromium dashboards include simple graphics for visual detection of these values.

\subsection{What is a flaky test?}
Flaky tests tend to occur on the system level. They do not often manifest in unit or integration tests since these tests usually simply call a function with some parameters and check its output against some expected value in isolation from the system as a whole (or, in the case of integration - against some external service).

System tests, on the other hand, target the software as a whole. Developers of software for mobile Operating Systems employ instrumentation to simulate user input and check for the presence of elements in the user interface. Essentially, a test can be distilled down to:
\begin{verbatim}
  Given I am in this state (arrange)
  When I do this (act)
  Then I am in this state (assert)
\end{verbatim}

Flaky tests are commonly caused by timing issues. The action step requires the arrange step to have completed before it runs; there must be some kind of wait and check mechanism to block further progress until the arrange step has finished.

For example, consider a test run that initially enters state $S_{1}$ (arrange), where $S_{1}$ is a vector $(x_{1},x_{2},x_{3})$. Action $A$ transforms $S_1$ to $S_2$ --- $(x_{1},x_{2},x_{3})$. Finally, the assertion checks that the current state is equal to $S_2$.

If each value of the vector $S_{1}$ is loaded asynchronously, the test must check for each of the vectors values to be correct before $A$ is applied. If the test checked only the first two values, the action would complete only if the third element happened to have loaded before the values in position 1 and 2.

In practice, with hundreds of user interface elements, these types of problems can become quite prevalent. Often, the solutions are obvious, but sometimes there is not enough information to solve the problem.
