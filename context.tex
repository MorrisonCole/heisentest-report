% This chapter should cover background information, related work, research done and tools or software selected for use in the project.
% You should not include well-known things (e.g., HTML or Java) or try to give tutorials on how to use a tool or code library (use references to books and websites for that information). Everything you include should be directly relevant to your work and the relationship made clear.
% This chapter is likely to be fairly substantial, perhaps 8-10 pages.
\section{Context}
\label{sec:context}

\begin{mdframed}
	\begin{itemize}
		\item Provide the necessary context and background information to describe how your project relates to what is already known or available.
		\item If relevant, a survey of similiar solutions, programs or applications to yours, and how yours is differentiated.
		\item A description of the research carried out to learn about the nature of the problem(s) being investigated and potential solutions. The form of the research will vary widely depending on the kind of project. For example, it might involve searching through research publications and online resources, or might involve an exploration of design possibilities for a user interface or program structure.
		\item Outline and reference the sources of information you are drawing on (papers, books, websites, etc.). State how each relates to your work.
		\item Introduce the software, programming languages, library code, frameworks and other tools that you are using. Discuss choices and make clear which you made use of and why.
	\end{itemize}
\end{mdframed}


\subsection{Test Driven Development}

Normally, test suites are written with the intention of (amongst other things):
\begin{itemize}
	\item Shortening a product's delivery cycle.
	\item Reducing a product's defects.
\end{itemize}

A good test suite achieves this by exercising chunks of the program's code to ensure it behaves in some expected way. At a high level, each test can be broken into three steps - arrangement, action and assertion.

To a developer actively practising Test Driven Development, an unexpected failure usually manifests as a cue to take a step back and rethink the implementation of their solution to the problem being tackled.

With Test Driven Development being taken to new heights with Integration and System testing, it is rarely so simple.

Anecdotally, the higher-level the behaviour being tested, the more likely a test is to fail due to inputs unaccounted for. Tests that exercise valid code can fail unexpectedly and unpredictably.

Developers working with such tests face a dilemma --- \'surpress\' the tests until they are fixed, or acknowledge them but keep them active, potentially randomly breaking the build from time to time until they are fixed.

Since the tests exercise behaviour which is needed (and indeed, works as intended), both solutions are workarounds until the tests can be resolved.

\subsection{Why are Flaky Tests a problem?}

Consider a project with small team of developers. They write Acceptance, Integration and Unit tests to cover every feature they add to the software. Their test suite $T$, is made up of tests $t_{0}, t_{1}, \dots, t_{n}$. On average, the tests take 20 minutes to run and each developer is expected to run the entire suite locally before pushing a new feature to the development branch. The test suite contains a number of flaky Acceptance tests $F$ written $f_{0}, f_{1}, \dots, f_{k} \text{, where $k \leq n$}$ that the team are aware of. $F \subseteq T$.

A developer is working on a new feature and has written an Acceptance test to ensure it works as expected. The Acceptance test passes, so the developer prepares to push the code onto the development branch. Before doing so, the developer must run the test suite locally to ensure the work did not break any existing functionality. The test suite runs, but 4 of the Acceptance tests --- $t_{3}, t_{7}, t_{9} \text{ and } t_{21}$ --- fail.

The developer is sure they’ve seen $t_{3}, t_{7} \text{ and } t_{9}$ fail randomly before, i.e. $t_{3}, t_{7}, t_{9} \in F$. After checking with colleagues, they assume that $t_{21} \in F$, since it appears to be unrelated to the area of code they’ve been working on. The developer pushes the code.

In this example, $t_{21}$ was not actually flaky, but had been broken by the developers changes. It is important to recognise that, whilst in some situations it may be possible to run the test suite multiple times to be sure, sometimes this is impractical. Test suites may take hours, days or even months to run in their entirety, and may contain many flaky tests.

After the code is pushed, a different set of tests fail on the build server: $t_{3}, t_{7} \text{ and } t_{9}$ are now passing, but $t_{21}$ has once again failed. In this case, it is obvious that $t_{21}$ must be investigated, so the developer’s changeset is rolled back whilst they fix the change.

This may appear to be workable, but time was almost certainly wasted. The developer writing the new feature only became aware that they had broken existing functionality after the second run revealed a trend. If $t_{21}$ had failed alone, the developer would have taken action before committing the changeset.

When a subset of tests fail randomly each time the suite is run, it is all too easy for developers to begin ignoring real failures if and when they do occur.

\subsection{Living with Flaky Tests}

When the latest changes are due to be released, the flaky tests must be considered. Normally, a particular software iteration would be held back until all tests pass, but with flaky tests thrown into the mix - it may take multiple runs before all the tests pass at once - again, adding overhead.

The set of flaky tests cause confusion, obscure real failures and erode trust.

\subsection{Why not just ‘suppress’ the flaky tests?}

Informally, let a valid test be a test that asserts that some unit of the application behaves as expected. A redundant test might check behaviour that is not required, or is tested thoroughly elsewhere. Assuming all tests in a suite are valid, then any flaky tests in the suite are also valid. A flaky test may cover behaviour that is not tested elsewhere. Suppressing (temporarily retiring) the flaky test may do more harm than good, since the behaviour it previously tested will now be go completely unchecked unless manually tested upon each commit.

\subsection{Existing Tools}

There are continuous integration tools that aim to mitigate flaky test impact. Namely, the Chromium Flakiness Dashboard, Shazam’s internal ‘Flaky Test Monitor’ and various Jenkins plugins. Typically, these attach a ‘flakiness’ value to each test case that represents the likelihood of failure during any given test run. Some also expose statistics such as stability (the tests flakiness across the previous n runs), successful runs since last failure and first known failure. Both the Shazam Android and Chromium dashboards include simple graphics for visual detection of these values.

\subsection{What is a flaky test?}
Flaky tests tend to occur on the system level. They do not often manifest in unit or integration tests since these tests usually simply call a function with some parameters and check its output against some expected value in isolation from the system as a whole (or, in the case of integration - against some external service).

Acceptance tests, on the other hand, target the software as a whole. Developers of software for mobile Operating Systems employ instrumentation to simulate user input and check for the presence of elements in the user interface. Essentially, a test can be distilled down to:
\begin{verbatim}
  Given I am in this state (arrange)
  When I do this (act)
  Then I am in this state (assert)
\end{verbatim}

Flaky tests are commonly caused by timing issues. The action step requires the arrange step to have completed before it runs; there must be some kind of wait and check mechanism to block further progress until the arrange step has finished.

For example, consider a test run that initially enters state $S_{1}$ (arrange), where $S_{1}$ is a vector $(x_{1},x_{2},x_{3})$. Action $A$ transforms $S_1$ to $S_2$ --- $(x_{1},x_{2},x_{3})$. Finally, the assertion checks that the current state is equal to $S_2$.

If each value of the vector $S_{1}$ is loaded asynchronously, the test must check for each of the vectors values to be correct before $A$ is applied. If the test checked only the first two values, the action would complete only if the third element happened to have loaded before the values in position 1 and 2.

In practice, with hundreds of user interface elements, these types of problems can become quite prevalent. Often, the solutions are obvious, but sometimes there is not enough information to solve the problem.

\subsection{This is a real problem}

\todo{Heisentest is an answer to a real problem faced by myself and members of my team at Shazam. Despite limited personal experience in industry, it is obvious that others face the same issue. Mozilla, Shazam and Chromium have known flaky tests. Android even includes an @Flaky annotation in its testing kit for automatic rerunning of known troublesome tests.}
